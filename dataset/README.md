# ğŸ—‚ï¸ Dataset Preparation for LISAT

This README provides links and organization instructions for all datasets required to train and evaluate **LISAT** models across referring segmentation, reasoning segmentation, and captioning tasks.

---

## ğŸ“¦ 1. Referring Segmentation Datasets  
*Used for training and evaluating referring segmentation tasks (e.g., refcoco family).*

- ğŸ“„ Annotations:  
  [FP-/R- RefCOCO (+/g)](https://drive.google.com/file/d/1mA3kcY3QiAZz1Zr89MCKYd7e3LBIwUzl/view?usp=sharing)

- ğŸ–¼ COCO 2014 Images:  
  [train2014.zip](http://images.cocodataset.org/zips/train2014.zip)

---

## ğŸ§  2. Visual Question Answering Dataset  
*Used for pretraining referring segmentation models.*

- ğŸ”¤ LLaVA-Instruct-150k:  
  [HuggingFace Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_instruct_150k.json)

---

## ğŸ§© 3. Semantic Segmentation Datasets  
*Used to train segmentation models for reasoning tasks (multi-class or fine-grained part segmentation).*

- [ADE20K Challenge Dataset](http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip)  
- [COCO-Stuff](http://calvin.inf.ed.ac.uk/wp-content/uploads/data/cocostuffdataset/stuffthingmaps_trainval2017.zip)  
- [PACO-LVIS (annotations only)](https://github.com/facebookresearch/paco/tree/main#dataset-setup)  
- [PASCAL-Part](https://github.com/facebookresearch/VLPart/tree/main/datasets#pascal-part)  
- [COCO Images 2017](http://images.cocodataset.org/zips/train2017.zip)

> âš ï¸ Notes:
> - For COCO-Stuff, use `stuffthingmaps_trainval2017.zip` only.
> - Only the LVIS subset of PACO is required.
> - Place COCO images under `dataset/coco/train2017/`.

---

## ğŸ”€ 4. Augmented Reasoning Dataset  
*Includes false-premise queries for robust reasoning segmentation, geospatial reasoning segmentation.*

- [FP-Aug ReasonSeg (Google Drive)](https://drive.google.com/file/d/11WNg1KaV2mk7gTdJRa2aahGqfj4luTDw/view?usp=sharing)
- [vqa_caption (Google Drive)](https://drive.google.com/drive/folders/1S9Z4IVLyoWND5C8iSjFLUFDQCCvb6bCI?usp=sharing)
- [GeoReasonSeg (Huggingface)](https://huggingface.co/datasets/jquenum/GRES/blob/main/README.md)
---

## ğŸ“ Folder Structure

After downloading all files, organize them as follows:

```
LISAT
â”œâ”€â”€ dataset
â”‚   â”œâ”€â”€ ade20k
â”‚   â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â””â”€â”€ images
â”‚   â”œâ”€â”€ coco
â”‚   â”‚   â””â”€â”€ train2017
â”‚   â”‚       â”œâ”€â”€ 000000000009.jpg
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ cocostuff
â”‚   â”‚   â””â”€â”€ train2017
â”‚   â”‚       â”œâ”€â”€ 000000000009.png
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ llava_dataset
â”‚   â”‚   â””â”€â”€ llava_instruct_150k.json
â”‚   â”œâ”€â”€ reason_seg
â”‚   â”‚   â”œâ”€â”€ ReasonSeg
â”‚   â”‚   â”‚   â”œâ”€â”€ train
â”‚   â”‚   â”‚   â””â”€â”€ val
â”‚   â”‚   â””â”€â”€ GeoReasonSeg
â”‚   â”‚       â”œâ”€â”€ train
â”‚   â”‚       â””â”€â”€ val
â”‚   â”œâ”€â”€ refer_seg
â”‚   â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”‚   â””â”€â”€ mscoco
â”‚   â”‚   â”‚       â””â”€â”€ images
â”‚   â”‚   â”‚           â””â”€â”€ train2014
â”‚   â”‚   â”œâ”€â”€ refclef
â”‚   â”‚   â”œâ”€â”€ refcoco
â”‚   â”‚   â”œâ”€â”€ refcoco+
â”‚   â”‚   â”œâ”€â”€ refcocog
â”‚   â”‚   â”œâ”€â”€ R-refcoco
â”‚   â”‚   â”œâ”€â”€ R-refcoco+
â”‚   â”‚   â”œâ”€â”€ R-refcocog
â”‚   â”‚   â”œâ”€â”€ fprefcoco
â”‚   â”‚   â”œâ”€â”€ fprefcoco+
â”‚   â”‚   â””â”€â”€ fprefcocog
â”‚   â”œâ”€â”€ vqa_caption
â”‚   â”‚   â”œâ”€â”€ Conversation_Complex_Reasoning.jsonl
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ vlpart
â”‚       â”œâ”€â”€ paco
â”‚       â”‚   â””â”€â”€ annotations
â”‚       â””â”€â”€ pascal_part
â”‚           â”œâ”€â”€ train.json
â”‚           â””â”€â”€ VOCdevkit
```

---

## ğŸ§­ Tips

- Use symbolic links (`ln -s`) if you want to avoid dataset duplication across projects.
- Always verify image paths and annotation alignment before launching training or evaluation scripts.

---

## ğŸ“¬ Questions?

Feel free to reach out via [issues](https://github.com/lisat-bair/LISAt/issues) if you encounter any trouble setting up the datasets.
